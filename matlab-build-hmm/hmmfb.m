% HMMFB.M (c) 2009 by Sridhar Sunderam
% Forward and backward algorithms for hidden Markov models
% Compute probability that a sequence of observations y was generated by a
% specific hidden Markov model H.
% Follows exposition by Andrew Fraser in "Hidden Markov models and
% dynamical systems", SIAM 2009.
function [Ly,alpha,gamma,beta] = hmmfb(y,H)

switch nargin
    case 0
        % HMM
        mu{1} = [1 -1]; Sigma{1} = [.9 .4; .4 .3];
        mu{2} = [0 0]; Sigma{2} = [1 0; 0 1];
        P0 = [0.25 0.75]';           % State priors
        TR = [0.3 0.7; 0.6 0.4];    % Transition probabilities
        
        % Simulated observations
        x1 = mvnrnd(mu{1}, Sigma{1}, 100);
        x2 = mvnrnd(mu{2}, Sigma{2}, 100);
        y = [x1;x2];
    case 1
        % HMM
        mu{1} = [1 -1]; Sigma{1} = [.9 .4; .4 .3];
        mu{2} = [0 0]; Sigma{2} = [1 0; 0 1];
        P0 = [0.25 0.75]';           % State priors
        TR = [0.3 0.7; 0.6 0.4];    % Transition probabilities
    otherwise
        % Extract variables from struct H
        fn = fieldnames(H);
        for i=1:length(fn)
            eval([fn{i} ' = getfield(H,fn{i});']);
        end
end

% Forward algorithm to compute the likelihood of an observation sequence
% given the model.
nS = size(P0,1);    % no. of states
T = size(y,1);     % no. of time steps

% Precompute values of P(y|s) given the form of the pdf
Py = zeros(nS,T);  % preallocate
for i=1:nS
    Py(i,:) = mvnpdf(y,mu{i},Sigma{i});
end

Ps = zeros(nS,T);
Psy = zeros(nS,T);
gamma = zeros(T,1);
alpha = zeros(nS,T);

% Initial time step
z1 = Py(:,1).*P0;           % P(y[1]& s) = P(y[1]|s)*P(s)    
gamma(1) = sum(z1);         % P(y[1]) = sum_s(P(y[1]& s)
alpha(:,1) = z1/gamma(1);   % P(s|y[1]) = P(y[1]|s)*P(s)/P(y[1])

% Repeat for all time steps
for t = 2:T
    % P(s[t]|y[1:t-1]) = prob. of transition to s from any state given previous observations
    Ps(:,t) = TR'*alpha(:,t-1);     % P(s|y[1:t-1]) = sum_s'( P(s|s')*P(s'|y[1:t-1]) )
    
    Psy(:,t) = Py(:,t).*Ps(:,t);    % P(s & y[i] | y[1:i-1])
    gamma(t) = sum(Psy(:,t));       % P(y[t]|y[1:t-1]) = sum_s( P(s & y[t] | y[1:t-1]) )
    alpha(:,t) = Psy(:,t)/gamma(t); % P(s|y(1:t)) = P(y(1:t)|s)*P(s)/P(y(1:t))
end

% Probability of the observations under model H
% P(y[1:T]|H) = P(y[1:T-1])*P(y[T]|y[1:T-1])    Expansion using Bayes rule
%                   = P(y[1:T-2])*P(y[T-1]|y[1:T-2])*P(y[T]|y[1:T-1])
%       = P(y[1])* PRODUCT_t(P(y[t]|y[1:t-1])) = PRODUCT_t(gamma[t])
% Taking the log on both sides gives the likelihood
Ly = sum(log(gamma));

% Backward procedure
beta = zeros(nS,T);
beta(:,T) = 1;
for t = T:-1:2
    beta(:,t-1) = TR*(beta(:,t).*Py(:,t))/gamma(t);
end

return